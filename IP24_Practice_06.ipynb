{"cells":[{"cell_type":"markdown","source":["\n","# **[군장병 AI 기본1] Image Processing**\n","\n","* ### Hak Gu Kim, Ph.D.\n","  * ### Assistant Professor\n","  * ### Graduate School of Advanced Imaging Science, Multimedia & Film (GSAIM)\n","  * ### Chung-Ang University\n","  * ### Webpage: www.irislab.cau.ac.kr\n"],"metadata":{"id":"17h2G6wX9AMS"}},{"cell_type":"markdown","source":["# **Programming Practice VI: Convolutional Neural Networks (CNNs)**\n","\n","### 1. Follow and understand the examples in the code: Practice VI-1 ~ VI-3\n","### 2. Design your own CNNs: Practice VI-4 (e.g., more conv or fc layers, various kernel sizes, etc.)\n","### 3. Train and test your CNNs model on CIFAR-10 dataset (Try to achieve your best performance!)"],"metadata":{"id":"DsqcUtsx9Db5"}},{"cell_type":"markdown","source":["## **[Step 0]** Environmental Setting"],"metadata":{"id":"55KCnbql9Oma"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from torch.utils.data import DataLoader\n","\n","import torchvision\n","import torchvision.transforms as transforms"],"metadata":{"id":"lfseYMx1JKxy","executionInfo":{"status":"ok","timestamp":1727077467899,"user_tz":-540,"elapsed":15259,"user":{"displayName":"이교석","userId":"04913314646302460109"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## **[Practice VI-1]** Dataset"],"metadata":{"id":"O30LdlatWDqv"}},{"cell_type":"markdown","source":["## 1-1) MNIST Dataset\n","\n","The MNIST dataset consists of 70,000 28x28 handwritten digits images in 10 classess. 60,000 images for training and 10,000 images for test.\n","\n","- http://yann.lecun.com/exdb/mnist/\n","- https://pytorch.org/vision/stable/generated/torchvision.datasets.KMNIST.html#torchvision.datasets.KMNIST"],"metadata":{"id":"Eu6xu3AcCvWU"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"8j2ldCEJYk-P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727077472803,"user_tz":-540,"elapsed":4906,"user":{"displayName":"이교석","userId":"04913314646302460109"}},"outputId":"99e79740-43a5-4cb1-81c2-8a9a8becdab1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 13251370.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 482164.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 4421263.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 12095573.82it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# MNIST Dataset\n","mnist_train = torchvision.datasets.MNIST(root='./', train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n","mnist_test  = torchvision.datasets.MNIST(root='./', train=False, transform=transforms.ToTensor(), target_transform=None, download=True)\n","mnist_train, mnist_val = torch.utils.data.random_split(mnist_train, [50000, 10000])\n","\n","# Data Loader for MNIST\n","mnist_train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True)\n","mnist_val_loader   = DataLoader(mnist_val, batch_size=128, shuffle=False)\n","mnist_test_loader  = DataLoader(mnist_test, batch_size=128, shuffle=False)"]},{"cell_type":"markdown","source":["## 1-2) CIFAR-10\n","\n","The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n","\n","- https://www.cs.toronto.edu/~kriz/cifar.html\n","- https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10"],"metadata":{"id":"ENgvafX9Cy2K"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"HGcXIcCW6M3O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727077480375,"user_tz":-540,"elapsed":7573,"user":{"displayName":"이교석","userId":"04913314646302460109"}},"outputId":"2cd854cf-3bba-491d-9823-12f531e3c041"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:03<00:00, 48314571.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./cifar-10-python.tar.gz to ./\n","Files already downloaded and verified\n"]}],"source":["# Define the Transforms for Training Dataset\n","transforms_train = transforms.Compose([\n","  transforms.RandomCrop(32, padding=4),\n","  transforms.RandomHorizontalFlip(),\n","  transforms.ToTensor(),\n","  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","# Define the Transforms for Testing Dataset\n","transforms_test = transforms.Compose([\n","  transforms.ToTensor(),\n","  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","# CIFAR-10 Dataset\n","cifar_train = torchvision.datasets.CIFAR10(root='./', train=True, download=True, transform=transforms_train)\n","cifar_test = torchvision.datasets.CIFAR10(root='./', train=False, download=True, transform=transforms_test)\n","\n","# Data Loader for CIFAR-10\n","# cifar_train_loader = DataLoader(cifar_train, batch_size=128, shuffle=True)\n","# cifar_test_loader = DataLoader(cifar_test, batch_size=128, shuffle=False)\n","cifar_train_loader = DataLoader(cifar_train, batch_size=128, shuffle=True, num_workers=2)\n","cifar_test_loader = DataLoader(cifar_test, batch_size=128, shuffle=False, num_workers=2)"]},{"cell_type":"markdown","source":["## **[Practice VI-2]** (Practice) Implement Each Component of CNNs\n","- Convolutional Layer\n","- Batch Normalization\n","- Dropout Layer"],"metadata":{"id":"5KkNTDf7C33J"}},{"cell_type":"markdown","source":["## 2-1) Convolutional Layer\n","\n","`nn.Conv2d`: Applies a 2D convolution over an input signal composed of several input planes.\n","\n","- https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"],"metadata":{"id":"l8MUOO-FDRqc"}},{"cell_type":"markdown","source":["**Parameters for** `nn.Conv2d`\n","- in_channels (int) – Number of channels in the input image\n","\n","- out_channels (int) – Number of channels produced by the convolution\n","\n","- kernel_size (int or tuple) – Size of the convolution filter (kernel)\n","\n","- stride (int or tuple, optional) – Stride of the convolution (Default: `1`)\n","\n","- padding (int, tuple or str, optional) – Padding added to boundaries of the input (Default: `0`)\n","\n","- padding_mode (string, optional) – `zeros`, `reflect`, `replicate` or `circular` (Default: `zeros`)\n","\n","- dilation (int or tuple, optional) – Spacing between kernel elements (Default: `1`)"],"metadata":{"id":"Yz_lhYRNDW4c"}},{"cell_type":"markdown","source":["**Examples**\n","- With square kernels and equal stride:\n","\n","  `conv_layer = nn.Conv2d(16, 33, 3, stride=2)`\n","\n","- non-square kernels and unequal stride and with padding:\n","\n","  `conv_layer = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))`\n","\n","- non-square kernels and unequal stride and with padding and dilation:\n","\n","  `conv_layer = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))`"],"metadata":{"id":"SReKGxZADY9K"}},{"cell_type":"code","source":["# Example of convolutional layer\n","\n","# Input dimension: 1 x 3 x 32 x 32\n","# Convolutional layer: 32 5x5 filters with stride 2, padding 2\n","\n","x = torch.randn(1, 3, 32, 32) # input: x\n","\n","conv_layer = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=2, padding=2)\n","\n","print('Input size:\\n', x.size())\n","print()\n","print('Output size:\\n', conv_layer(x).size())"],"metadata":{"id":"SjfkJOIDDbHf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727077480881,"user_tz":-540,"elapsed":508,"user":{"displayName":"이교석","userId":"04913314646302460109"}},"outputId":"bb8eb963-5f7a-4c1f-bf5f-c311a136a5b6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Input size:\n"," torch.Size([1, 3, 32, 32])\n","\n","Output size:\n"," torch.Size([1, 32, 16, 16])\n"]}]},{"cell_type":"markdown","source":["## 2-2) Batch Normalization\n","\n","`nn.BatchNorm2d`: Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension)\n","\n","- https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n","- S. Ioffe and C. Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, **ICML 2015** [[Link]](http://proceedings.mlr.press/v37/ioffe15.html)"],"metadata":{"id":"vnierdJ9DcUH"}},{"cell_type":"markdown","source":["**Parameters** for `nn.BatchNorm2d`\n","- num_features – $C$ from an expected input of size ($N, C, H, W$)"],"metadata":{"id":"u7a0k7xaDfn8"}},{"cell_type":"markdown","source":["**Example**\n","\n","- With learnable parameters\n","\n","  `bn = nn.BatchNorm2d(100)`\n"],"metadata":{"id":"NvJ_LX65Dhzs"}},{"cell_type":"code","source":["# Batch Normalization\n","\n","x = torch.randn(1, 3, 32, 32)\n","\n","bn = nn.BatchNorm2d(num_features=3)\n","\n","print('Input size:\\n', x.size())\n","print()\n","print('Size of feature after BN:\\n', bn(x).size()) # Please check the output size after the batch normalization whether the size of input is changed or not"],"metadata":{"id":"NGrwIrjdDoN-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727077480881,"user_tz":-540,"elapsed":4,"user":{"displayName":"이교석","userId":"04913314646302460109"}},"outputId":"5b5851d7-fa7f-493e-dab4-ccdbb9ceeed9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Input size:\n"," torch.Size([1, 3, 32, 32])\n","\n","Size of feature after BN:\n"," torch.Size([1, 3, 32, 32])\n"]}]},{"cell_type":"markdown","source":["## **[Practice VI-3]** (Practice) Build Simple Convolutional Neural Networks\n","\n","- `nn.Sequential`: A sequential container. Modules will be added to it in the order they are passed in the constructor.\n","- https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"],"metadata":{"id":"7483tUe4DkF9"}},{"cell_type":"markdown","source":["## 3-1) Define CNNs Architecture\n","\n","- 2 conv layers with 7x7 kernel (Convolution + Batch normalization + ReLU)\n","- 1 fc layer for 10 classes"],"metadata":{"id":"PYljB4n1EBS4"}},{"cell_type":"code","source":["# Model: Simple Convolutional Neural Networks\n","\n","class ConvNet(nn.Module):\n","\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        # 1 input image channel, 32 output channels, 7x7 square convolution, 1 stride\n","        self.conv_layer1 = nn.Sequential(\n","            nn.Conv2d(1, 32, 7),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","        )\n","        # 32 input image channel, 64 output channels, 7x7 square convolution, 1 stride\n","        self.conv_layer2 = nn.Sequential(\n","            nn.Conv2d(32, 64, 7),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","        )\n","\n","        self.fc = nn.Linear(64*16*16, 10)\n","\n","    def forward(self, x):\n","        out_conv1 = self.conv_layer1(x)\n","        out_conv2 = self.conv_layer2(out_conv1)\n","        feature_1d = torch.flatten(out_conv2, 1)\n","        out = self.fc(feature_1d)\n","        return out\n"],"metadata":{"id":"svbOAiGQEiWa","executionInfo":{"status":"ok","timestamp":1727077480881,"user_tz":-540,"elapsed":3,"user":{"displayName":"이교석","userId":"04913314646302460109"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Using GPU\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","model = ConvNet()\n","model = model.to(device)"],"metadata":{"id":"qAZlEsqyEqic","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727077480882,"user_tz":-540,"elapsed":4,"user":{"displayName":"이교석","userId":"04913314646302460109"}},"outputId":"3b3c6fd7-0ddf-4f11-af41-8b1d16eea39b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"markdown","source":["## 3-2) Define Optimizer & Loss\n","- Optimization using stochastic gradient descent (SGD)\n","- Learning rate α=0.01\n","- Loss function: Cross Entropy Loss"],"metadata":{"id":"VbKT8XHjEh7K"}},{"cell_type":"code","source":["# Optimizer: Stochastic Gradient Descent Method\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"],"metadata":{"id":"d60kmY8VE0vH","executionInfo":{"status":"ok","timestamp":1727077480882,"user_tz":-540,"elapsed":3,"user":{"displayName":"이교석","userId":"04913314646302460109"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Define Loss function (Cross Entropy Loss here)\n","\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"9ag_vvvVFOdh","executionInfo":{"status":"ok","timestamp":1727077480882,"user_tz":-540,"elapsed":3,"user":{"displayName":"이교석","userId":"04913314646302460109"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["##3-3) Train the Simple CNNs Model\n","- Dataset: MNIST\n","- Epochs = 10"],"metadata":{"id":"MbxJl9jjFS5w"}},{"cell_type":"code","source":["# Train the model\n","total_step = len(mnist_train_loader)\n","epochs = 10\n","for epoch in range(epochs):\n","    for i, (images, labels) in enumerate(mnist_train_loader):  # mini batch for loop\n","\n","        # Upload to gpu\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = loss_fn(outputs, labels)\n","\n","        # Backward pass & Optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i+1) % 100 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n","                   .format(epoch+1, epochs, i+1, total_step, loss.item()))"],"metadata":{"id":"Udiwr4k_Ffnc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727077547960,"user_tz":-540,"elapsed":67080,"user":{"displayName":"이교석","userId":"04913314646302460109"}},"outputId":"44222d82-c596-4f5d-ae9d-5f5554275e46"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Step [100/391], Loss: 0.2013\n","Epoch [1/10], Step [200/391], Loss: 0.0862\n","Epoch [1/10], Step [300/391], Loss: 0.1353\n","Epoch [2/10], Step [100/391], Loss: 0.0499\n","Epoch [2/10], Step [200/391], Loss: 0.1057\n","Epoch [2/10], Step [300/391], Loss: 0.0522\n","Epoch [3/10], Step [100/391], Loss: 0.0478\n","Epoch [3/10], Step [200/391], Loss: 0.0293\n","Epoch [3/10], Step [300/391], Loss: 0.0461\n","Epoch [4/10], Step [100/391], Loss: 0.0567\n","Epoch [4/10], Step [200/391], Loss: 0.0890\n","Epoch [4/10], Step [300/391], Loss: 0.0249\n","Epoch [5/10], Step [100/391], Loss: 0.0287\n","Epoch [5/10], Step [200/391], Loss: 0.0292\n","Epoch [5/10], Step [300/391], Loss: 0.0146\n","Epoch [6/10], Step [100/391], Loss: 0.1934\n","Epoch [6/10], Step [200/391], Loss: 0.0316\n","Epoch [6/10], Step [300/391], Loss: 0.0458\n","Epoch [7/10], Step [100/391], Loss: 0.0257\n","Epoch [7/10], Step [200/391], Loss: 0.0298\n","Epoch [7/10], Step [300/391], Loss: 0.0191\n","Epoch [8/10], Step [100/391], Loss: 0.0304\n","Epoch [8/10], Step [200/391], Loss: 0.0039\n","Epoch [8/10], Step [300/391], Loss: 0.0107\n","Epoch [9/10], Step [100/391], Loss: 0.0229\n","Epoch [9/10], Step [200/391], Loss: 0.0464\n","Epoch [9/10], Step [300/391], Loss: 0.0053\n","Epoch [10/10], Step [100/391], Loss: 0.0136\n","Epoch [10/10], Step [200/391], Loss: 0.0316\n","Epoch [10/10], Step [300/391], Loss: 0.0084\n"]}]},{"cell_type":"markdown","source":["##3-4) Test the Trained CNNs Model"],"metadata":{"id":"4ShRnU-tFrZX"}},{"cell_type":"code","source":["# Test the model\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in mnist_test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(images)\n","\n","        _, predicted = torch.max(outputs.data, 1)  # classificatoin model -> get the label prediction of top 1\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Accuracy of Simple CNN on MNIST test set: {} %'.format(100 * correct / total))"],"metadata":{"id":"qZs4OU6sF0U1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727077549140,"user_tz":-540,"elapsed":1181,"user":{"displayName":"이교석","userId":"04913314646302460109"}},"outputId":"fd9d46e5-ead2-4eaa-b429-c4c88d1821ef"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of Simple CNN on MNIST test set: 98.95 %\n"]}]},{"cell_type":"markdown","source":["## **[Practice VI-4]** Design Your Own Convolutional Neural Networks\n","**References**\n","\n","https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n","\n","https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n","\n","\n","**Options**\n","- The number of convolutional layers\n","- Stride & padding & dilation\n","- Various activation functions\n","- Pooling layers (max pool, avg pool)\n","- The number of fully connected layers\n","- The dimension of hidden layers\n","- The size of kernels at each layer\n","- *etc*."],"metadata":{"id":"pEEgEYmRF4J4"}},{"cell_type":"code","source":["# Change the following CNNs architecture\n","\n","class myConvNet(nn.Module):\n","\n","    def __init__(self):\n","        super(myConvNet, self).__init__()\n","        # 3 input image channel, 32 output channels, 7x7 square convolution, 1 stride\n","        self.conv_layer1 = nn.Sequential(\n","            nn.Conv2d(3, 32, 7),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","        )\n","        # 32 input image channel, 64 output channels, 7x7 square convolution, 1 stride\n","        self.conv_layer2 = nn.Sequential(\n","            nn.Conv2d(32, 64, 7),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","        )\n","\n","        self.fc = nn.Linear(64*20*20, 10)\n","\n","    def forward(self, x):\n","        out_conv1 = self.conv_layer1(x)\n","        out_conv2 = self.conv_layer2(out_conv1)\n","        feature_1d = torch.flatten(out_conv2, 1)\n","        out = self.fc(feature_1d)\n","        return out\n"],"metadata":{"id":"caCYEDDiJV4N","executionInfo":{"status":"ok","timestamp":1727077549140,"user_tz":-540,"elapsed":4,"user":{"displayName":"이교석","userId":"04913314646302460109"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","model = myConvNet()\n","model = model.to(device)"],"metadata":{"id":"vto9LzS_Jymy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727077549140,"user_tz":-540,"elapsed":3,"user":{"displayName":"이교석","userId":"04913314646302460109"}},"outputId":"8c860d90-f4f3-4afc-f6af-65a525c5b435"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"markdown","source":["##4-1) Train Your CNNs Model\n","You can change the number of epochs, learning rate, optimizer, *etc*."],"metadata":{"id":"jHrFQ1rVKDad"}},{"cell_type":"code","source":["# Optimizer: Stochastic Gradient Descent Method\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","\n","# Define Loss function\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"Zt25_qGLJ2LV","executionInfo":{"status":"ok","timestamp":1727077549140,"user_tz":-540,"elapsed":2,"user":{"displayName":"이교석","userId":"04913314646302460109"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","total_step = len(cifar_train_loader)\n","epochs = 5\n","for epoch in range(epochs):\n","    for i, (images, labels) in enumerate(cifar_train_loader):  # mini batch for loop\n","\n","        # Upload to gpu\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = loss_fn(outputs, labels)\n","\n","        # Backward pass & Optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i+1) % 100 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n","                   .format(epoch+1, epochs, i+1, total_step, loss.item()))"],"metadata":{"id":"Gt-Va_0NKRhb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727077640001,"user_tz":-540,"elapsed":90863,"user":{"displayName":"이교석","userId":"04913314646302460109"}},"outputId":"2f0e76bc-62b9-4f5f-cb27-642cc111b94c"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/5], Step [100/391], Loss: 2.4427\n","Epoch [1/5], Step [200/391], Loss: 1.7762\n","Epoch [1/5], Step [300/391], Loss: 1.5699\n","Epoch [2/5], Step [100/391], Loss: 1.5409\n","Epoch [2/5], Step [200/391], Loss: 1.4649\n","Epoch [2/5], Step [300/391], Loss: 1.3350\n","Epoch [3/5], Step [100/391], Loss: 1.3291\n","Epoch [3/5], Step [200/391], Loss: 1.2763\n","Epoch [3/5], Step [300/391], Loss: 1.0532\n","Epoch [4/5], Step [100/391], Loss: 1.2048\n","Epoch [4/5], Step [200/391], Loss: 1.1706\n","Epoch [4/5], Step [300/391], Loss: 1.2099\n","Epoch [5/5], Step [100/391], Loss: 1.1842\n","Epoch [5/5], Step [200/391], Loss: 1.0101\n","Epoch [5/5], Step [300/391], Loss: 1.2540\n"]}]},{"cell_type":"markdown","source":["##4-2) Test the Trained Your CNNs Model\n","Try to acheive the best performance!"],"metadata":{"id":"SdzqgVFuKmKn"}},{"cell_type":"code","source":["# Test the model\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in cifar_test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(images)\n","\n","        _, predicted = torch.max(outputs.data, 1)  # classificatoin model -> get the label prediction of top 1\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Accuracy of Your CNNs on CIFAR-10 test set: {} %'.format(100 * correct / total))"],"metadata":{"id":"g9cJL-QpKq4F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727077642346,"user_tz":-540,"elapsed":2346,"user":{"displayName":"이교석","userId":"04913314646302460109"}},"outputId":"912639d5-9609-427a-8c20-c45510dbbcd0"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of Your CNNs on CIFAR-10 test set: 60.94 %\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"y9p4hd6cde-p","executionInfo":{"status":"ok","timestamp":1727077642346,"user_tz":-540,"elapsed":4,"user":{"displayName":"이교석","userId":"04913314646302460109"}}},"execution_count":16,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}