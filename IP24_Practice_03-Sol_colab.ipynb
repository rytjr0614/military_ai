{"cells":[{"cell_type":"markdown","source":["\n","# **[군장병 AI 기본1] Image Processing**\n","\n","* ### Hak Gu Kim, Ph.D.\n","  * ### Assistant Professor\n","  * ### Graduate School of Advanced Imaging Science, Multimedia & Film (GSAIM)\n","  * ### Chung-Ang University\n","  * ### Webpage: www.irislab.cau.ac.kr\n"],"metadata":{"id":"QSKlCRkoHiL8"}},{"cell_type":"markdown","source":["# **Programming Practice III: Panorama Stitching**\n","\n","### 1. Understande SIFT descriptor\n","### 2. Implement RANSAC algorithm with SIFT descriptor features\n","### 3. Perform your own panorama images using two view images"],"metadata":{"id":"9F5DyIzWHvE0"}},{"cell_type":"markdown","source":["## **[Step 0]** Environmental Setting"],"metadata":{"id":"cZUui5N9OzIj"}},{"cell_type":"code","source":["!pip install opencv-contrib-python==4.10.0.84"],"metadata":{"id":"6P6dLTDyebqb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724578233582,"user_tz":-540,"elapsed":3496,"user":{"displayName":"Hak Gu Kim","userId":"14893909611994281750"}},"outputId":"a44d43c2-4681-4609-fb09-7c34ac7704d4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-contrib-python==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-contrib-python==4.10.0.84) (1.26.4)\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"9wQvfpAnhB3T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724578183346,"user_tz":-540,"elapsed":39726,"user":{"displayName":"Hak Gu Kim","userId":"14893909611994281750"}},"outputId":"f1c716d0-13e8-4134-cf5f-aedc82ad8051"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# Connect to the google drive #\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["# Import the required libraries for image processing\n","\n","import sys\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import numpy as np\n","import math\n","import random\n","from tqdm.notebook import tqdm\n","plt.rcParams['figure.figsize'] = [15, 15]\n","\n","# Define the directory\n","dir = '/content/gdrive/My Drive/Colab Notebooks/' # File path"],"metadata":{"id":"OZ4vG1FEJZPG","executionInfo":{"status":"ok","timestamp":1724579308260,"user_tz":-540,"elapsed":889,"user":{"displayName":"Hak Gu Kim","userId":"14893909611994281750"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Define the functions for the load and save the input image\n","\n","def loadImg(in_fname):\n","  img = cv2.imread(dir + in_fname)\n","\n","  if img is None:\n","    print('Image load failed!')\n","    sys.exit()\n","\n","  plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","  plt.title(\"Input RGB Image\")\n","  plt.show()\n","\n","  return img\n","\n","## Save image file\n","def saveImg(out_img, out_fname):\n","  cv2.imwrite(dir + out_fname, out_img)"],"metadata":{"id":"zfV0T9_PJmNm","executionInfo":{"status":"ok","timestamp":1724578246570,"user_tz":-540,"elapsed":698,"user":{"displayName":"Hak Gu Kim","userId":"14893909611994281750"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Define the functions for panorama image generation\n","\n","##############################################\n","#          Transformation Matrix, H          #\n","##############################################\n","def homography(pairs):\n","    rows = []\n","    for i in range(pairs.shape[0]):\n","        p1 = np.append(pairs[i][0:2], 1)\n","        p2 = np.append(pairs[i][2:4], 1)\n","        row1 = [0, 0, 0, p1[0], p1[1], p1[2], -p2[1]*p1[0], -p2[1]*p1[1], -p2[1]*p1[2]]\n","        row2 = [p1[0], p1[1], p1[2], 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1], -p2[0]*p1[2]]\n","        rows.append(row1)\n","        rows.append(row2)\n","\n","    rows = np.array(rows)\n","    U, s, V = np.linalg.svd(rows)\n","    H = V[-1].reshape(3, 3)\n","    H = H/H[2, 2] # standardize to let w*H[2,2] = 1\n","    return H\n","\n","\n","##############################################\n","#         Panoramic Image Generation         #\n","##############################################\n","def stitchImg(left, right, H):\n","    print(\"stiching image ...\")\n","\n","    # Convert to double and normalize. Avoid noise.\n","    left = cv2.normalize(left.astype('float'), None,\n","                            0.0, 1.0, cv2.NORM_MINMAX)\n","    # Convert to double and normalize.\n","    right = cv2.normalize(right.astype('float'), None,\n","                            0.0, 1.0, cv2.NORM_MINMAX)\n","\n","    # left image\n","    height_l, width_l, channel_l = left.shape\n","    corners = [[0, 0, 1], [width_l, 0, 1], [width_l, height_l, 1], [0, height_l, 1]]\n","    corners_new = [np.dot(H, corner) for corner in corners]\n","    corners_new = np.array(corners_new).T\n","    x_news = corners_new[0] / corners_new[2]\n","    y_news = corners_new[1] / corners_new[2]\n","    y_min = min(y_news)\n","    x_min = min(x_news)\n","\n","    translation_mat = np.array([[1, 0, -x_min], [0, 1, -y_min], [0, 0, 1]])\n","    H = np.dot(translation_mat, H)\n","\n","    # Get height, width\n","    height_new = int(round(abs(y_min) + height_l))\n","    width_new = int(round(abs(x_min) + width_l))\n","    size = (width_new, height_new)\n","\n","    # right image\n","    warped_l = cv2.warpPerspective(src=left, M=H, dsize=size)\n","\n","    height_r, width_r, channel_r = right.shape\n","\n","    height_new = int(round(abs(y_min) + height_r))\n","    width_new = int(round(abs(x_min) + width_r))\n","    size = (width_new, height_new)\n","\n","\n","    warped_r = cv2.warpPerspective(src=right, M=translation_mat, dsize=size)\n","\n","    black = np.zeros(3)  # Black pixel.\n","\n","    # Stitching procedure, store results in warped_l.\n","    for i in tqdm(range(warped_r.shape[0])):\n","        for j in range(warped_r.shape[1]):\n","            pixel_l = warped_l[i, j, :]\n","            pixel_r = warped_r[i, j, :]\n","\n","            if not np.array_equal(pixel_l, black) and np.array_equal(pixel_r, black):\n","                warped_l[i, j, :] = pixel_l\n","            elif np.array_equal(pixel_l, black) and not np.array_equal(pixel_r, black):\n","                warped_l[i, j, :] = pixel_r\n","            elif not np.array_equal(pixel_l, black) and not np.array_equal(pixel_r, black):\n","                warped_l[i, j, :] = (pixel_l + pixel_r) / 2\n","            else:\n","                pass\n","\n","    stitch_image = warped_l[:warped_r.shape[0], :warped_r.shape[1], :]\n","    return stitch_image\n","\n","\n","def plot_sift(gray_img, rgb_img, keypnt):\n","    tmp_img = rgb_img.copy()\n","    sift_on_img = cv2.drawKeypoints(gray_img, keypnt, tmp_img, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n","    return sift_on_img\n","\n","\n","def plot_matches(matches, total_img):\n","    match_img = total_img.copy()\n","    offset = total_img.shape[1]/2\n","    fig, ax = plt.subplots()\n","    ax.set_aspect('equal')\n","    ax.imshow(np.array(match_img).astype('uint8')) #　RGB is integer type\n","\n","    ax.plot(matches[:, 0], matches[:, 1], 'xr')\n","    ax.plot(matches[:, 2] + offset, matches[:, 3], 'xr')\n","\n","    ax.plot([matches[:, 0], matches[:, 2] + offset], [matches[:, 1], matches[:, 3]],\n","            'r', linewidth=0.5)\n","\n","    plt.show()"],"metadata":{"id":"aMxocwNO5Rle","executionInfo":{"status":"ok","timestamp":1724578752512,"user_tz":-540,"elapsed":1078,"user":{"displayName":"Hak Gu Kim","userId":"14893909611994281750"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## **[Practice III-1]** Understande SIFT descriptor feature and matching\n","* Following codes are SIFT Feature Extraction and Matching\n","* There is nothing for you to do in this field"],"metadata":{"id":"9SJED_18PZFr"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"8j2ldCEJYk-P","executionInfo":{"status":"ok","timestamp":1724579171437,"user_tz":-540,"elapsed":1089,"user":{"displayName":"Hak Gu Kim","userId":"14893909611994281750"}}},"outputs":[],"source":["##############################################\n","#             Feature Extraction             #\n","##############################################\n","def SIFT(img):\n","    siftDetector= cv2.xfeatures2d.SIFT_create() # limit 1000 points\n","    # siftDetector= cv2.SIFT_create()  # depends on OpenCV version\n","\n","    kp, des = siftDetector.detectAndCompute(img, None)\n","    return kp, des  # keypoints & descriptors\n","\n","\n","##############################################\n","#               Feature Matching             #\n","##############################################\n","def siftMatch(kp1, des1, img1, kp2, des2, img2, threshold):\n","    bf = cv2.BFMatcher()\n","    matches = bf.knnMatch(des1,des2, k=2)\n","\n","    # Apply ratio test\n","    good = []\n","    for m,n in matches:\n","        if m.distance < threshold*n.distance:\n","            good.append([m])\n","\n","    matches = []\n","    for pair in good:\n","        matches.append(list(kp1[pair[0].queryIdx].pt + kp2[pair[0].trainIdx].pt))\n","\n","    matches = np.array(matches)\n","    return matches\n","\n"]},{"cell_type":"markdown","source":["## ## **[Practice III-2]** Implement RANSAC algorithm with SIFT descriptor features\n","* This is main part in this practice\n","* Carefully read and understand the following RANSAC code,\n","* Then please **complete the incomplete code:** `varable_name = [ ]`"],"metadata":{"id":"iq9FG-I6PFd-"}},{"cell_type":"code","source":["##############################################\n","#                   RANSAC                   #\n","##############################################\n","def ransac(matches, k_samples, threshold, iters):\n","    num_best_inliers = 0\n","\n","    for i in range(iters):\n","        rand_pnt = randomPoint(matches, k_samples)\n","        H = homography(rand_pnt)\n","\n","        # Avoid dividing by zero\n","        if np.linalg.matrix_rank(H) < 3:\n","            continue\n","\n","        errs = getError(matches, H)\n","        idx = np.where(errs < threshold)[0]\n","        inliers = matches[idx]\n","\n","        num_inliers = len(inliers)\n","        if num_inliers > num_best_inliers:\n","            ############################################\n","            ##       COMPLETE THE FOLLOWING CODE      ##\n","            ############################################\n","            # Find the best inliers and transformation matrix, H\n","            best_inliers = inliers.copy()\n","            num_best_inliers = num_inliers\n","            best_H = H.copy()\n","\n","    print(\"inliers/matches: {}/{}\".format(num_best_inliers, len(matches)))\n","    return best_inliers, best_H\n","\n","def randomPoint(matches, k_samples):\n","  ############################################\n","  ##       COMPLETE THE FOLLOWING CODE      ##\n","  ############################################\n","  # k_samples: the number of random points\n","  # Randomly sample the K well-matched keypoints (i.e., matches)\n","  # Fianlly return the randomly sampled well-matched keypoints (i.e., matches), rand_pnt\n","  idx = random.sample(range(len(matches)), k_samples)\n","  rand_pnt = [matches[i] for i in idx ]\n","\n","  return np.array(rand_pnt)\n","\n","def getError(rand_pnt, H):\n","    num_points = len(rand_pnt)\n","    all_p1 = np.concatenate((rand_pnt[:, 0:2], np.ones((num_points, 1))), axis=1)\n","    all_p2 = rand_pnt[:, 2:4]\n","\n","    est_p2 = np.zeros((num_points, 2))\n","    for i in range(num_points):\n","        ############################################\n","        ##       COMPLETE THE FOLLOWING CODE      ##\n","        ############################################\n","        # Calculate the estimated p2\n","        # The estimated p2 = H*p1\n","        temp_est_p2 = np.dot(H, all_p1[i])\n","\n","        est_p2[i] = (temp_est_p2/temp_est_p2[2])[0:2] # set index 2 to 1 and slice the index 0, 1\n","\n","    ############################################\n","    ##       COMPLETE THE FOLLOWING CODE      ##\n","    ############################################\n","    # Compute error\n","    errors = np.linalg.norm(all_p2 - est_p2 , axis=1) ** 2\n","\n","    return errors"],"metadata":{"id":"H5W3_DY3PF3R","executionInfo":{"status":"ok","timestamp":1724579239221,"user_tz":-540,"elapsed":569,"user":{"displayName":"Hak Gu Kim","userId":"14893909611994281750"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## **[Practice III-3]** Perform your own panorama images using two view images\n","\n","* Try to change various variables such as the number of samples, threshold, the number of iterations in RANSAC function\n","* Try to get your own panorama image with the pictures you took"],"metadata":{"id":"NNKzsdjhBnTu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGcXIcCW6M3O"},"outputs":[],"source":["##############################################\n","#                Main Function               #\n","##############################################\n","\n","if __name__ == \"__main__\":\n","\n","    fname_left = 'img_left.jpg'\n","    fname_right = 'img_right.jpg'\n","\n","    img_left  = cv2.resize(cv2.imread(dir + fname_left), dsize=(720, 720))\n","    img_right = cv2.resize(cv2.imread(dir + fname_right), dsize=(720, 720))\n","\n","    gray_left = cv2.cvtColor(img_left, cv2.COLOR_RGB2GRAY)\n","    gray_right = cv2.cvtColor(img_right, cv2.COLOR_RGB2GRAY)\n","\n","    kp_left, des_left = SIFT(gray_left)\n","    kp_right, des_right = SIFT(gray_right)\n","\n","    print(len(kp_left))   # The number of keypoints on the left image\n","    print(des_left.shape) # The 128-dimensional descriptor for each keypoint\n","\n","    kp_left_img = plot_sift(gray_left, img_left, kp_left)\n","    kp_right_img = plot_sift(gray_right, img_right, kp_right)\n","    #total_kp = np.concatenate((kp_left_img, kp_right_img), axis=1)\n","    #plt.imshow(total_kp)\n","\n","    matches = siftMatch(kp_left, des_left, img_left, kp_right, des_right, img_right, 0.5)   # The coordinates of the matched correspondences p1=[x, y] and p2=[x', y']\n","    temp_img = np.concatenate((img_left, img_right), axis=1)\n","    plot_matches(matches, temp_img) # Good mathces\n","\n","    print(matches.shape)  # The number of well-matched keypoints (i.e., correspondences)\n","\n","\n","    inliers, H = ransac(matches, 400, 0.5, 1000)\n","    plot_matches(inliers, temp_img) # show inliers matches\n","\n","    panoramic_img = stitchImg(img_left, img_right, H)*255\n","    cv2_imshow(panoramic_img)\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyNa4HcGdJ/JMXKajGNq9/XB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}